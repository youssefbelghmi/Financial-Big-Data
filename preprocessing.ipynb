{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917fd307-c849-469d-80da-ad0bfa5d3b9e",
   "metadata": {},
   "source": [
    "# Notebook for Preprocessing\n",
    "This notebook focuses on the preprocessing of financial market data, specifically Parquet files containing stock information for S&P 500 companies. The preprocessing steps are designed to clean, transform, and enhance the raw data, ensuring it is ready for analysis and modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29caa1e8-b822-426f-b0ab-085b294ba6cb",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries. Key libraries include :\n",
    "- `numpy` and `pandas`: For numerical calculations and efficient DataFrame manipulations.\n",
    "- `random` and `os`: For file and folder operations and random selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d4c7d-d666-4ca3-89e8-a5cb16f00162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d15d6a-aecd-4ff0-89d3-045c0895e581",
   "metadata": {},
   "source": [
    "The following function preprocesses parquet files containing stock market data for S&P 500 companies. The goal is to clean and structure the data, making it suitable for further financial analysis. Given the messy and inconsistent nature of financial data, this function ensures consistency and quality while adding essential metrics to analyze market behavior and dynamics. It ultimately provides a structured dataset tailored for future advanced applications.\n",
    "\n",
    "The function processes the data through the following steps :\n",
    "\n",
    "1) Data Extraction : Ensures the raw data is loaded and the temporal information is properly structured.\n",
    "- Reads Parquet files for each company from the input folder.  \n",
    "- Extracts and formats the `date` and `time` from the `index` column.  \n",
    "\n",
    "2) Filtering : Focuses on market hours removes noise from off-hour trading.\n",
    "- Retains only rows within US market hours (09:30 to 16:00 EST).  \n",
    "\n",
    "3) Renaming and Cleaning : Provides a consistent and interpretable dataset.\n",
    "- Renames columns (e.g., `X.Open` â†’ `bid_price`) for clarity and consistency.  \n",
    "- Removes rows with missing values in critical fields like `bid_price` and `ask_price`.  \n",
    "\n",
    "4) Feature Engineering : Calculates additional metrics to enrich the dataset.\n",
    "- `mid_price`: The average of bid and ask prices.  \n",
    "- `order_density`: The total volume of bid and ask orders.  \n",
    "- `order_imbalance`: Measures the imbalance between bid and ask volumes.  \n",
    "- `spread`: The absolute difference between ask and bid prices, measuring market liquidity.\n",
    "- `vw_spread`: Volume-weighted spread to reflect liquidity costs.   \n",
    "- `relative_spread`: The spread relative to the mid_price, providing a normalized measure of liquidity.  \n",
    "- `log_return`: Logarithmic returns to capture price changes.  \n",
    "- `volatility`: Rolling standard deviation of log returns to assess market variability.\n",
    "\n",
    "5) Saving Processed Data : Stores the preprocessed data for downstream analysis and modeling.\n",
    "- Exports the cleaned and enhanced dataset as CSV files to the specified output folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stocks_parquet_files(input_folder, output_folder, volatility_window=10):\n",
    "    \"\"\"\n",
    "    Preprocess Parquet files for each company's ticker in the S&P500 folder:\n",
    "    - Extract data from the 'index' column into 'date' and 'time' (properly formatted).\n",
    "    - Filter rows based on time range (09:30:00-05:00 to 16:00:00-05:00).\n",
    "    - Rename columns for consistency.\n",
    "    - Add calculated features: mid_price, order_density, order_imbalance, spread, vw_spread, relative_spread, log_return, volatility.\n",
    "    - Remove rows with missing values in bid/ask columns.\n",
    "    - Remove rows where spread is negative.\n",
    "    - Save the preprocessed DataFrame as CSV in the specified output folder.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing company tickers with Parquet files.\n",
    "        output_folder (str): Path to save the preprocessed CSV files.\n",
    "        volatility_window (int): Rolling window size for volatility calculation.\n",
    "\n",
    "    Returns:\n",
    "        None: Saves preprocessed CSV files in the output folder.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # List all tickers (subfolders) in the input directory, sorted alphabetically\n",
    "    tickers = sorted([d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))])\n",
    "\n",
    "    for ticker in tickers:\n",
    "        ticker_folder = os.path.join(input_folder, ticker)\n",
    "        output_csv = os.path.join(output_folder, f\"{ticker}_2010_cleaned.csv\")\n",
    "\n",
    "        print(f\"Processing ticker: {ticker}\")\n",
    "\n",
    "        # List all Parquet files in the ticker's folder\n",
    "        parquet_files = [\n",
    "            os.path.join(ticker_folder, f) for f in os.listdir(ticker_folder)\n",
    "            if f.endswith('.parquet') and f.startswith('2010')\n",
    "        ]\n",
    "\n",
    "        if not parquet_files:\n",
    "            print(f\"No Parquet files for 2010 found for ticker: {ticker}\")\n",
    "            continue\n",
    "\n",
    "        # Read and concatenate all 2010 Parquet files\n",
    "        dataframes = []\n",
    "        for parquet_file in sorted(parquet_files):  # Ensure files are processed in date order\n",
    "            df = pd.read_parquet(parquet_file)\n",
    "            dataframes.append(df)\n",
    "\n",
    "        if dataframes:\n",
    "            full_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "            # Ensure the 'index' column is present and convert to datetime\n",
    "            full_dataframe['index'] = pd.to_datetime(full_dataframe['index'], errors='coerce')\n",
    "\n",
    "            # Split 'index' into 'date' and 'time' columns\n",
    "            full_dataframe['date'] = full_dataframe['index'].dt.date\n",
    "            full_dataframe['time'] = full_dataframe['index'].dt.strftime('%H:%M:%S%z')\n",
    "            \n",
    "            # Fix timezone formatting\n",
    "            full_dataframe['time'] = full_dataframe['time'].str[:-5] + full_dataframe['time'].str[-5:-2] + ':' + full_dataframe['time'].str[-2:]\n",
    "            full_dataframe.drop(columns=['index'], inplace=True)  # Remove the original index column\n",
    "\n",
    "            # Reorder columns to place 'date' and 'time' first\n",
    "            cols = ['date', 'time'] + [col for col in full_dataframe.columns if col not in ['date', 'time']]\n",
    "            full_dataframe = full_dataframe[cols]\n",
    "\n",
    "            # Filter rows based on time range (09:30:00-05:00 to 16:00:00-05:00)\n",
    "            full_dataframe = full_dataframe[\n",
    "                (full_dataframe['time'] >= '09:30:00-05:00') & (full_dataframe['time'] <= '16:00:00-05:00')\n",
    "            ]\n",
    "\n",
    "            # Rename columns\n",
    "            full_dataframe.rename(\n",
    "                columns={\n",
    "                    'X.Open': 'bid_price',\n",
    "                    'X.High': 'bid_volume',\n",
    "                    'X.Low': 'ask_price',\n",
    "                    'X.Close': 'ask_volume'\n",
    "                },\n",
    "                inplace=True\n",
    "            )\n",
    "\n",
    "            # Drop rows with missing values in bid/ask columns\n",
    "            full_dataframe.dropna(subset=['bid_price', 'bid_volume', 'ask_price', 'ask_volume'], inplace=True)\n",
    "\n",
    "            # Add calculated columns\n",
    "            full_dataframe['mid_price'] = (full_dataframe['bid_price'] + full_dataframe['ask_price']) / 2\n",
    "            full_dataframe['order_density'] = full_dataframe['bid_volume'] + full_dataframe['ask_volume']\n",
    "            full_dataframe['order_imbalance'] = (full_dataframe['bid_volume'] - full_dataframe['ask_volume']) / (full_dataframe['bid_volume'] + full_dataframe['ask_volume'])\n",
    "            full_dataframe['spread'] = full_dataframe['ask_price'] - full_dataframe['bid_price']\n",
    "            full_dataframe['vw_spread'] = full_dataframe['spread'] * full_dataframe['order_density']\n",
    "            full_dataframe['relative_spread'] = full_dataframe['spread'] / full_dataframe['mid_price']\n",
    "            \n",
    "            # Handle zeros or NaNs in mid_price to avoid log errors\n",
    "            full_dataframe['log_return'] = np.where(\n",
    "                (full_dataframe['mid_price'] > 0) & (full_dataframe['mid_price'].shift(1) > 0),\n",
    "                np.log(full_dataframe['mid_price'] / full_dataframe['mid_price'].shift(1)),\n",
    "                np.nan\n",
    "            )\n",
    "            full_dataframe['volatility'] = full_dataframe['log_return'].rolling(window=volatility_window, min_periods=1).std()\n",
    "\n",
    "            # Remove rows where spread is negative\n",
    "            full_dataframe = full_dataframe[full_dataframe['spread'] >= 0]\n",
    "\n",
    "            # Save the preprocessed DataFrame to CSV\n",
    "            full_dataframe.to_csv(output_csv, index=False)\n",
    "            print(f\"Preprocessed CSV saved: {output_csv}\")\n",
    "        else:\n",
    "            print(f\"No data for 2010 for ticker: {ticker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths for input and output folders\n",
    "    input_folder = \"S&P500\"  # Folder containing company subfolders with Parquet files\n",
    "    output_folder = \"S&P500_cleaned\"  # Folder to save preprocessed CSV files\n",
    "\n",
    "    # Preprocess all Parquet files for S&P500\n",
    "    preprocess_stocks_parquet_files(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd707958-8266-4d42-a9c5-f3f15b016833",
   "metadata": {},
   "source": [
    "Randomly selecting and displaying a cleaned CSV file provides a quick way to verify that previous preprocessing steps were applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9caa4141-393f-46f6-9d2a-1c8f32429f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying CSV for FRT_2010_cleaned.csv:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_volume</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>ask_volume</th>\n",
       "      <th>mid_price</th>\n",
       "      <th>order_density</th>\n",
       "      <th>order_imbalance</th>\n",
       "      <th>spread</th>\n",
       "      <th>vw_spread</th>\n",
       "      <th>relative_spread</th>\n",
       "      <th>log_return</th>\n",
       "      <th>volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>09:31:00-05:00</td>\n",
       "      <td>67.76</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.96</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.860</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>09:32:00-05:00</td>\n",
       "      <td>68.01</td>\n",
       "      <td>11.0</td>\n",
       "      <td>68.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.025</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>09:33:00-05:00</td>\n",
       "      <td>68.06</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.065</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>09:34:00-05:00</td>\n",
       "      <td>68.07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>68.14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68.105</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.001063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>09:35:00-05:00</td>\n",
       "      <td>68.08</td>\n",
       "      <td>7.0</td>\n",
       "      <td>68.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.115</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.001015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75820</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>15:52:00-05:00</td>\n",
       "      <td>77.96</td>\n",
       "      <td>7.0</td>\n",
       "      <td>77.97</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.965</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75821</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>15:53:00-05:00</td>\n",
       "      <td>78.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75822</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>15:54:00-05:00</td>\n",
       "      <td>78.06</td>\n",
       "      <td>5.0</td>\n",
       "      <td>78.08</td>\n",
       "      <td>3.0</td>\n",
       "      <td>78.070</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.000887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75823</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>15:55:00-05:00</td>\n",
       "      <td>78.12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>78.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>78.120</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75824</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>15:58:00-05:00</td>\n",
       "      <td>78.04</td>\n",
       "      <td>11.0</td>\n",
       "      <td>78.04</td>\n",
       "      <td>3.0</td>\n",
       "      <td>78.040</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75825 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date            time  bid_price  bid_volume  ask_price  \\\n",
       "0      2010-01-04  09:31:00-05:00      67.76        10.0      67.96   \n",
       "1      2010-01-04  09:32:00-05:00      68.01        11.0      68.04   \n",
       "2      2010-01-04  09:33:00-05:00      68.06         8.0      68.07   \n",
       "3      2010-01-04  09:34:00-05:00      68.07         5.0      68.14   \n",
       "4      2010-01-04  09:35:00-05:00      68.08         7.0      68.15   \n",
       "...           ...             ...        ...         ...        ...   \n",
       "75820  2010-12-31  15:52:00-05:00      77.96         7.0      77.97   \n",
       "75821  2010-12-31  15:53:00-05:00      78.00         8.0      78.00   \n",
       "75822  2010-12-31  15:54:00-05:00      78.06         5.0      78.08   \n",
       "75823  2010-12-31  15:55:00-05:00      78.12         5.0      78.12   \n",
       "75824  2010-12-31  15:58:00-05:00      78.04        11.0      78.04   \n",
       "\n",
       "       ask_volume  mid_price  order_density  order_imbalance  spread  \\\n",
       "0             2.0     67.860           12.0         0.666667    0.20   \n",
       "1             1.0     68.025           12.0         0.833333    0.03   \n",
       "2             1.0     68.065            9.0         0.777778    0.01   \n",
       "3             3.0     68.105            8.0         0.250000    0.07   \n",
       "4             1.0     68.115            8.0         0.750000    0.07   \n",
       "...           ...        ...            ...              ...     ...   \n",
       "75820         2.0     77.965            9.0         0.555556    0.01   \n",
       "75821         1.0     78.000            9.0         0.777778    0.00   \n",
       "75822         3.0     78.070            8.0         0.250000    0.02   \n",
       "75823         2.0     78.120            7.0         0.428571    0.00   \n",
       "75824         3.0     78.040           14.0         0.571429    0.00   \n",
       "\n",
       "       vw_spread  relative_spread  log_return  volatility  \n",
       "0           2.40         0.002947         NaN         NaN  \n",
       "1           0.36         0.000441    0.002429         NaN  \n",
       "2           0.09         0.000147    0.000588    0.001302  \n",
       "3           0.56         0.001028    0.000588    0.001063  \n",
       "4           0.56         0.001028    0.000147    0.001015  \n",
       "...          ...              ...         ...         ...  \n",
       "75820       0.09         0.000128    0.000128    0.000757  \n",
       "75821       0.00         0.000000    0.000449    0.000804  \n",
       "75822       0.16         0.000256    0.000897    0.000887  \n",
       "75823       0.00         0.000000    0.000640    0.000932  \n",
       "75824       0.00         0.000000    0.000192    0.000523  \n",
       "\n",
       "[75825 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_random_cleaned_csv(folder_path):\n",
    "    \"\"\"\n",
    "    Randomly selects and displays a cleaned CSV file from the specified folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing cleaned CSV files.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the content of the randomly selected CSV.\n",
    "    \"\"\"\n",
    "    # List all files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    # Randomly select a CSV file\n",
    "    random_csv = random.choice(csv_files)\n",
    "    csv_path = os.path.join(folder_path, random_csv)\n",
    "\n",
    "    # Load and display the CSV\n",
    "    print(f\"Displaying CSV for {random_csv}:\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    display(df)\n",
    "\n",
    "# Specify the folder containing cleaned CSV files\n",
    "cleaned_folder = \"S&P500_cleaned\"\n",
    "\n",
    "# Display a random cleaned CSV\n",
    "display_random_cleaned_csv(cleaned_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21bc63d-618b-4713-883e-962688bfb165",
   "metadata": {},
   "source": [
    "We are now guaranteed that the cleaned data is structured and ready for analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
